{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "\u001b[K    100% |████████████████████████████████| 286kB 20.6MB/s \n",
      "\u001b[31mfastai 1.0.52 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[31mthinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: joblib\n",
      "Successfully installed joblib-0.13.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'nhanes' from '/home/ec2-user/SageMaker/nhanes.py'>\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import glob\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import importlib\n",
    "import nhanes as nhanes\n",
    "importlib.reload(nhanes)\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "print(nhanes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/home/orpgol/OpportunisticLearning/CDC/NHANES/'\n",
    "DATA_PATH = 'CDC/NHANES/'\n",
    "DATASET = 'arthritis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "The code below loads each dataset: dataset_features, dataset_targets\n",
    "\n",
    "Here, all datasets are defined explicitly (see nhanes.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processing: IMQ_H.XPT(92062, 199)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nhanes)\n",
    "ds = nhanes.Dataset(DATA_PATH)\n",
    "ds.load_arthritis()\n",
    "n_fe = ds.features.shape[1]\n",
    "n_classes = 2\n",
    "\n",
    "indx = np.argwhere(ds.targets != 3)\n",
    "dataset_features_og = ds.features[indx.flatten()]\n",
    "dataset_targets = ds.targets[indx.flatten()]\n",
    "\n",
    "print(type(dataset_features_og))\n",
    "print(type(dataset_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(nhanes)\n",
    "# print((dataset_features.shape))\n",
    "# print((dataset_targets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = dataset_features_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=25)\n",
    "# dataset_features = copy.deepcopy(dataset_features_og)\n",
    "# dataset_features = pca.fit_transform(dataset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3533013334279683\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "n_comp = 10\n",
    "svd = decomposition.TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd.fit(dataset_features_og)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "dataset_features = copy.deepcopy(dataset_features_og)\n",
    "dataset_features = svd.transform(dataset_features)\n",
    "# test_features = svd.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nhanes)\n",
    "perm = np.random.permutation(dataset_targets.shape[0])\n",
    "dataset_features = dataset_features[perm]\n",
    "dataset_targets = dataset_targets[perm]\n",
    "\n",
    "def get_batch(n_size, phase):\n",
    "    # select indices\n",
    "    n_samples = dataset_features.shape[0]\n",
    "    n_classes = int(dataset_targets.max() + 1)\n",
    "    if phase == 'test':\n",
    "        inds_sel = np.arange(0, int(n_samples*0.15), 1)\n",
    "    elif phase == 'validation':\n",
    "        n_samples = dataset_features.shape[0]\n",
    "        inds_sel = np.arange(int(n_samples*0.15), int(n_samples*0.30), 1)\n",
    "    elif phase == 'train':\n",
    "        n_samples = dataset_features.shape[0]\n",
    "        inds_sel = np.arange(int(n_samples*0.30), n_samples, 1)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    inds_sel = np.random.permutation(inds_sel)\n",
    "    batch_inds = []\n",
    "    for cl in range(n_classes):\n",
    "        inds_cl = inds_sel[dataset_targets[inds_sel] == cl]\n",
    "        batch_inds.extend(inds_cl[:n_size//n_classes])\n",
    "    batch_inds = np.random.permutation(batch_inds)\n",
    "    \n",
    "    return dataset_features[batch_inds], dataset_targets[batch_inds]\n",
    "    \n",
    "features_trn, targets_trn = get_batch(n_size=20000, phase='train')\n",
    "features_tst, targets_tst = get_batch(n_size=10000, phase='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier 0.8826232985681457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.00      0.01       657\n",
      "           1       0.88      1.00      0.94      5000\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      5657\n",
      "   macro avg       0.56      0.50      0.47      5657\n",
      "weighted avg       0.81      0.88      0.83      5657\n",
      "\n",
      "[[   3  654]\n",
      " [  10 4990]]\n",
      "Support Vector Classifier 0.592893759943433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.39      0.18       657\n",
      "           1       0.89      0.62      0.73      5000\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      5657\n",
      "   macro avg       0.50      0.50      0.46      5657\n",
      "weighted avg       0.80      0.59      0.67      5657\n",
      "\n",
      "[[ 255  402]\n",
      " [1901 3099]]\n",
      "Logistic Regression 0.5059218667138059\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.50      0.19       657\n",
      "           1       0.89      0.51      0.64      5000\n",
      "\n",
      "   micro avg       0.51      0.51      0.51      5657\n",
      "   macro avg       0.50      0.51      0.42      5657\n",
      "weighted avg       0.80      0.51      0.59      5657\n",
      "\n",
      "[[ 331  326]\n",
      " [2469 2531]]\n",
      "Linear Discriminant Analysis 0.8838607035531201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       657\n",
      "           1       0.88      1.00      0.94      5000\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      5657\n",
      "   macro avg       0.44      0.50      0.47      5657\n",
      "weighted avg       0.78      0.88      0.83      5657\n",
      "\n",
      "[[   0  657]\n",
      " [   0 5000]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nhanes)\n",
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "clf.fit(features_trn, targets_trn)\n",
    "preds_tst = clf.predict(features_tst)\n",
    "accu = np.mean(preds_tst==targets_tst)\n",
    "print('Random Forest Classifier', accu)\n",
    "# print(clf.feature_importances_)\n",
    "print(classification_report(targets_tst, preds_tst))\n",
    "cm = confusion_matrix(targets_tst, preds_tst)\n",
    "print(cm)\n",
    "\n",
    "clf = SVC(gamma='auto', class_weight='balanced')\n",
    "clf.fit(features_trn, targets_trn)\n",
    "preds_tst = clf.predict(features_tst)\n",
    "accu = np.mean(preds_tst==targets_tst)\n",
    "print('Support Vector Classifier', accu)\n",
    "print(classification_report(targets_tst, preds_tst))\n",
    "cm = confusion_matrix(targets_tst, preds_tst)\n",
    "print(cm)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=200, class_weight='balanced')\n",
    "clf.fit(features_trn, targets_trn)\n",
    "preds_tst = clf.predict(features_tst)\n",
    "accu = np.mean(preds_tst==targets_tst)\n",
    "print('Logistic Regression', accu)\n",
    "print(classification_report(targets_tst, preds_tst))\n",
    "cm = confusion_matrix(targets_tst, preds_tst)\n",
    "print(cm)\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='svd')\n",
    "clf.fit(features_trn, targets_trn)\n",
    "preds_tst = clf.predict(features_tst)\n",
    "accu = np.mean(preds_tst==targets_tst)\n",
    "print('Linear Discriminant Analysis', accu)\n",
    "print(classification_report(targets_tst, preds_tst))\n",
    "cm = confusion_matrix(targets_tst, preds_tst)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: plot_confusion_matrix(targets_tst,preds_tst,classes=['Yes Cancer', 'No Cancer'],title='Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
